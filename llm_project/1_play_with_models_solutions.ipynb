{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29ccd56",
   "metadata": {},
   "source": [
    "#### HuggingFace token\n",
    "This token is mandatory. Register on the website and generate one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e0e99ed-024f-47e2-a98e-16a3697040fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e03ac3c3-8626-4b93-9beb-2ffd8be2ed65",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506b23f-b0ef-4f4a-a79d-d19afa0fa8ff",
   "metadata": {},
   "source": [
    "### 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "009f4187-ffd6-43ba-b5f1-709b5a3dcf1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModel, TFBertForQuestionAnswering,TFAutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "501bb050-01d4-4558-9955-3ca9e02502ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "GPU_use = 0\n",
    "st = \"cuda:\"+str(GPU_use)\n",
    "print(torch.cuda.current_device())\n",
    "torch.cuda.set_device(GPU_use) \n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "\n",
    "#st = \"cpu\"\n",
    "#torch.device(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d12b7e-c5fd-413d-b19e-6d04bbafb84b",
   "metadata": {},
   "source": [
    "### 2. Choose the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "979400de-5fce-45cc-b434-4cadf7e155fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model = 'meta-llama/Meta-Llama-3.1-8B'\n",
    "#base_model = 'meta-llama/Llama-3.2-1B'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c838824-c121-4d29-ad85-e2d445c821af",
   "metadata": {},
   "source": [
    "### 3.Importing the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0fda458-1360-476c-aff4-d6f3343ae071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokr = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d16c3230-d8c7-4f2e-b48e-ffc4c2116e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tokr(\"ciao come stai?\",return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2051c998-6eba-4b20-a6d4-1a0421229a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,     66,  23332,   2586,    357,   2192,     30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80feb7a6-d50c-41ae-a3b0-c819f041965f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>ciao come stai?']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokr.batch_decode(toks['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3b518-f116-4a75-84d7-92760599ca5e",
   "metadata": {},
   "source": [
    "### 4. importing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb44a4b9-4ca7-4db7-8b59-daa098f7353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True,)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, device_map=GPU_use,\n",
    "                                             token=access_token,quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c0c46-1729-4440-9bcf-ac6563f00aea",
   "metadata": {},
   "source": [
    "### 5. setting the correct prompt (different for each models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e65b8027-83de-4f9e-8fb0-500bc24476e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = \"\"\"\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "USER: {}\n",
    "===\n",
    "{}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "def sql_prompt(d): return fmt.format(d[\"context\"], d[\"question\"])\n",
    "def question(table, quest):\n",
    "    tst = {'context' : '', 'question':''}\n",
    "    tst['context'] = table\n",
    "    tst['question'] = quest\n",
    "    return sql_prompt(tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddba88f-3362-4fd0-9a8b-d59fe895c15f",
   "metadata": {},
   "source": [
    "### 6. play with the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bea873d-de33-45bb-94fe-0b668bdb4fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13.5 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "q = 'Pick the object banana and place next to the apple.'\n",
    "t = 'table(apple x: -51, y: -427, z: 156; banana x: -162, y: 411, z: 58; pear x: 346, y: 340, z: 0; melon x: -36, y: -243, z: 323; cup x: 328, y: -59, z: 326; bowl x: -274, y: -334, z: 51; box x: -158, y: -13, z: 79; basket x: -437, y: 35, z: 0;)'\n",
    "\n",
    "\n",
    "\n",
    "#t =''\n",
    "#q = 'hi, how are you?'\n",
    "#print(question(t,q))\n",
    "test = question(t,q)\n",
    "toks = tokr(test, return_tensors=\"pt\")\n",
    "res = model.generate(**toks.to(st), max_new_tokens=100, top_p = 0).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eadd62a2-e0c0-40fe-b8d3-06134926f4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "USER: table(apple x: -51, y: -427, z: 156; banana x: -162, y: 411, z: 58; pear x: 346, y: 340, z: 0; melon x: -36, y: -243, z: 323; cup x: 328, y: -59, z: 326; bowl x: -274, y: -334, z: 51; box x: -158, y: -13, z: 79; basket x: -437, y: 35, z: 0;)\n",
      "===\n",
      "Pick the object banana and place next to the apple.\n",
      "ASSISTANT: table(apple x: -51, y: -427, z: 156; banana x: -162, y: 411, z: 58; pear x: 346, y: 340, z: 0; melon x: -36, y: -243, z: 323; cup x: 328, y: -59, z: 326; bowl x: -274, y: -334, z: 51; box\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "print(tokr.batch_decode(res)[0].replace(\"*\",\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e768f5e5-c196-4e1d-88ff-709811df0210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Convert to Hugging Face Dataset\n",
    "\n",
    "data_path = \"../dataset_creation/data.json\"\n",
    "\n",
    "import json\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "dataset = Dataset.from_list(data[:100000]).train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f40b5e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90000/90000 [00:20<00:00, 4420.46 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:02<00:00, 4481.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokr.pad_token = tokr.eos_token\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"Context: {c} Question: {q}\" for c, q in zip(examples[\"context\"], examples[\"question\"])]\n",
    "    outputs = examples[\"answer\"]\n",
    "    model_inputs = tokr(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokr(outputs, max_length=512, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7cf63a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'table(apple x: -468, y: -92, z: 77; banana x: -151, y: -207, z: 385; pear x: -153, y: 244, z: 34; melon x: 335, y: -382, z: 474; cup x: 232, y: 376, z: 183; bowl x: -420, y: -393, z: 38; box x: -379, y: 18, z: 346; basket x: -351, y: -298, z: 238;)',\n",
       " 'question': 'Pick the object pear and place it in the bowl.',\n",
       " 'answer': 'go to x: -153, y: 244, z: 34+30; go to x: -153, y: 244, z: 34; close the gripper; go to x: -420, y: -393, z: 38+10; open the gripper; go home;',\n",
       " 'input_ids': [128000,\n",
       "  2014,\n",
       "  25,\n",
       "  2007,\n",
       "  11718,\n",
       "  273,\n",
       "  865,\n",
       "  25,\n",
       "  482,\n",
       "  20304,\n",
       "  11,\n",
       "  379,\n",
       "  25,\n",
       "  482,\n",
       "  6083,\n",
       "  11,\n",
       "  1167,\n",
       "  25,\n",
       "  220,\n",
       "  2813,\n",
       "  26,\n",
       "  44196,\n",
       "  865,\n",
       "  25,\n",
       "  482,\n",
       "  9690,\n",
       "  11,\n",
       "  379,\n",
       "  25,\n",
       "  482,\n",
       "  12060,\n",
       "  11,\n",
       "  1167,\n",
       "  25,\n",
       "  220,\n",
       "  18695,\n",
       "  26,\n",
       "  38790,\n",
       "  865,\n",
       "  25,\n",
       "  482,\n",
       "  9800,\n",
       "  11,\n",
       "  379,\n",
       "  25,\n",
       "  220,\n",
       "  13719,\n",
       "  11,\n",
       "  1167,\n",
       "  25,\n",
       "  220,\n",
       "  1958,\n",
       "  26,\n",
       "  10804,\n",
       "  263,\n",
       "  865,\n",
       "  25,\n",
       "  220,\n",
       "  16596,\n",
       "  11,\n",
       "  379,\n",
       "  25,\n",
       "  482,\n",
       "  18781,\n",
       "  11,\n",
       "  1167,\n",
       "  25,\n",
       "  220,\n",
       "  21358,\n",
       "  26,\n",
       "  10747,\n",
       "  865,\n",
       "  25,\n",
       "  220,\n",
       "  12338,\n",
       "  11,\n",
       "  379,\n",
       "  25,\n",
       "  220,\n",
       "  18322,\n",
       "  11,\n",
       "  1167,\n",
       "  25,\n",
       "  220,\n",
       "  10750,\n",
       "  26,\n",
       "  19763,\n",
       "  865,\n",
       "  25,\n",
       "  482,\n",
       "  12819,\n",
       "  11,\n",
       "  379,\n",
       "  25,\n",
       "  482,\n",
       "  18252,\n",
       "  11,\n",
       "  1167,\n",
       "  25,\n",
       "  220,\n",
       "  1987,\n",
       "  26,\n",
       "  3830,\n",
       "  865,\n",
       "  25,\n",
       "  482,\n",
       "  19867,\n",
       "  11,\n",
       "  379,\n",
       "  25,\n",
       "  220,\n",
       "  972,\n",
       "  11,\n",
       "  1167,\n",
       "  25,\n",
       "  220,\n",
       "  18061,\n",
       "  26,\n",
       "  14351,\n",
       "  865,\n",
       "  25,\n",
       "  482,\n",
       "  18113,\n",
       "  11,\n",
       "  379,\n",
       "  25,\n",
       "  482,\n",
       "  17690,\n",
       "  11,\n",
       "  1167,\n",
       "  25,\n",
       "  220,\n",
       "  13895,\n",
       "  37274,\n",
       "  16225,\n",
       "  25,\n",
       "  20305,\n",
       "  279,\n",
       "  1665,\n",
       "  38790,\n",
       "  323,\n",
       "  2035,\n",
       "  433,\n",
       "  304,\n",
       "  279,\n",
       "  19763,\n",
       "  13,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [128000,\n",
       "  3427,\n",
       "  311,\n",
       "  865,\n",
       "  25,\n",
       "  482,\n",
       "  9800,\n",
       "  11,\n",
       "  379,\n",
       "  25,\n",
       "  220,\n",
       "  13719,\n",
       "  11,\n",
       "  1167,\n",
       "  25,\n",
       "  220,\n",
       "  1958,\n",
       "  10,\n",
       "  966,\n",
       "  26,\n",
       "  733,\n",
       "  311,\n",
       "  865,\n",
       "  25,\n",
       "  482,\n",
       "  9800,\n",
       "  11,\n",
       "  379,\n",
       "  25,\n",
       "  220,\n",
       "  13719,\n",
       "  11,\n",
       "  1167,\n",
       "  25,\n",
       "  220,\n",
       "  1958,\n",
       "  26,\n",
       "  3345,\n",
       "  279,\n",
       "  23854,\n",
       "  7067,\n",
       "  26,\n",
       "  733,\n",
       "  311,\n",
       "  865,\n",
       "  25,\n",
       "  482,\n",
       "  12819,\n",
       "  11,\n",
       "  379,\n",
       "  25,\n",
       "  482,\n",
       "  18252,\n",
       "  11,\n",
       "  1167,\n",
       "  25,\n",
       "  220,\n",
       "  1987,\n",
       "  10,\n",
       "  605,\n",
       "  26,\n",
       "  1825,\n",
       "  279,\n",
       "  23854,\n",
       "  7067,\n",
       "  26,\n",
       "  733,\n",
       "  2162,\n",
       "  26,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001,\n",
       "  128001]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f114f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch dataset format\n",
    "train_dataset = tokenized_dataset[\"train\"].with_format(\"torch\")\n",
    "eval_dataset = tokenized_dataset[\"test\"].with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "010c615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,  # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Modify for LLaMA if necessary\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Wrap the model with PEFT\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94334201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_llama\",  # Output directory\n",
    "    eval_strategy=\"epoch\",  # Evaluate after every epoch\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Use mixed precision for speed\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    push_to_hub=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3235a856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the model is in training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5f5aefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight: requires_grad=False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.0.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.1.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.2.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.3.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.4.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.5.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.6.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.7.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.8.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.9.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.10.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.11.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.12.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.13.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.14.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.15.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.16.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.16.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.16.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.17.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.17.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.17.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.18.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.18.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.18.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.19.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.19.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.19.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.20.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.20.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.20.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.21.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.21.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.21.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.22.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.22.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.22.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.22.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.23.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.23.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.23.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.23.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.24.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.24.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.24.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.24.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.25.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.25.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.25.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.25.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.26.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.26.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.26.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.26.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.27.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.27.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.27.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.27.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.28.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.28.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.28.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.28.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.29.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.29.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.29.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.29.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.30.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.30.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.30.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.30.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.k_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight: requires_grad=False\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.o_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.31.mlp.gate_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.31.mlp.up_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.31.mlp.down_proj.weight: requires_grad=False\n",
      "base_model.model.model.layers.31.input_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight: requires_grad=False\n",
      "base_model.model.model.norm.weight: requires_grad=False\n",
      "base_model.model.lm_head.weight: requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "# Check if parameters require gradients\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e7be9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/67500 [00:16<30:45:32,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0951, 'grad_norm': 6.730384349822998, 'learning_rate': 0.0002999777777777777, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/67500 [00:33<30:37:46,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9464, 'grad_norm': 1.34651517868042, 'learning_rate': 0.0002999333333333333, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 28/67500 [00:46<30:21:16,  1.62s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[0;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      5\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     processing_class\u001b[38;5;241m=\u001b[39mtokr,\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\RoboLLM\\RoboLLM_venv\\lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\RoboLLM\\RoboLLM_venv\\lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32md:\\RoboLLM\\RoboLLM_venv\\lib\\site-packages\\transformers\\trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3612\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\RoboLLM\\RoboLLM_venv\\lib\\site-packages\\accelerate\\accelerator.py:2237\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2237\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2238\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32md:\\RoboLLM\\RoboLLM_venv\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\RoboLLM\\RoboLLM_venv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\RoboLLM\\RoboLLM_venv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokr,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44827c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27774dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = './fine_tuned_model'\n",
    "#base_model = 'meta-llama/Llama-3.2-1B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df325f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokr = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c73a6fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True,)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, device_map=GPU_use,\n",
    "                                             token=access_token,quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c0b8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = \"\"\"\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "USER: {}\n",
    "===\n",
    "{}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "def sql_prompt(d): return fmt.format(d[\"context\"], d[\"question\"])\n",
    "def question(table, quest):\n",
    "    tst = {'context' : '', 'question':''}\n",
    "    tst['context'] = table\n",
    "    tst['question'] = quest\n",
    "    return sql_prompt(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c7b079f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 391 ms\n",
      "Wall time: 395 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "q = 'Pick the object banana and place next to the apple.'\n",
    "t = 'table(apple x: -51, y: -427, z: 156; banana x: -162, y: 411, z: 58; pear x: 346, y: 340, z: 0; melon x: -36, y: -243, z: 323; cup x: 328, y: -59, z: 326; bowl x: -274, y: -334, z: 51; box x: -158, y: -13, z: 79; basket x: -437, y: 35, z: 0;)'\n",
    "\n",
    "t = 'Lorem Ipsum'\n",
    "q = 'when was lorem ipsum invented? explain in detail'\n",
    "\n",
    "#t =''\n",
    "#q = 'hi, how are you?'\n",
    "#print(question(t,q))\n",
    "test = question(t,q)\n",
    "toks = tokr(test, return_tensors=\"pt\")\n",
    "res = model.generate(**toks.to(st), max_new_tokens=100, top_p = 0).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e691819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "USER: Lorem Ipsum\n",
      "===\n",
      "when was lorem ipsum invented? explain in detail\n",
      "ASSISTANT:<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "print(tokr.batch_decode(res)[0].replace(\"*\",\"\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoboLLM_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
